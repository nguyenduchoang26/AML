{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICAT3190, Module 5, Excercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satellite image classification\n",
    "\n",
    "Following satellite image is obtained from the Söderfjärden meteorite crater near Vaasa at 10:00 in 28.9.2019. The image is acquired from European Sentinell 2 satellite by means of multispectral imaging device (MSI). The multispectral camera has acquired the image using 13 different wavelength bands instead of three (RGB) in the normal camera. The image was searched and dowloaded using [Copernicus Open Access Hub](https://scihub.copernicus.eu/dhus/), and preprosessed by using ESA's [SNAP](http://step.esa.int/main/download/) tool.\n",
    "\n",
    "\n",
    "The bands used are \n",
    "\n",
    "| Band number | Band name | Wavelength | Region | Remarks |\n",
    "| ----------- | --------- | -----------| ------ | ------- |\n",
    "|  1 | B1  |   443 nm | Violet     | Chlorophyll-A |\n",
    "|  2 | B2  |   490 nm | Cyan       | |\n",
    "|  3 | B3  |   560 nm | Green      | |\n",
    "|  4 | B4  |   665 nm | Red        | Chlorophyll_A |\n",
    "|  5 | B5  |   705 nm | Red        | |\n",
    "|  6 | B6  |   740 nm | Red        | |\n",
    "|  7 | B7  |   783 nm | Deep red   | |\n",
    "|  8 | B8  |   842 nm | NIR        | |\n",
    "|  9 | B8A |   865 nm | NIR        | |\n",
    "| 10 | B9  |   945 nm | NIR        | |\n",
    "| 11 | B10 |  1375 nm | NIR        | |\n",
    "| 12 | B11 |  1610 nm | NIR        | |\n",
    "| 13 | B12 |  2190 nm | NIR        | |\n",
    "\n",
    "The channels listed above can be used for creating a natural looking RGB-image, as shown below.\n",
    "\n",
    "![machine_learning.svg](kuvat/Soderfjarden_image.png)\n",
    "\n",
    "Even though, only three channels are used for RGB image, all 13 can be usefull features for land type and crops classification. \n",
    "\n",
    "## Training data \n",
    "\n",
    "In the image above, an expert has manually segmented some areas of the image and labelled them according to land use. The labelled areas are:\n",
    "\n",
    "| Segment no. | Segment name | Segment color | \n",
    "| ----------- | --------- | -----------| \n",
    "|  1 | Water      | Violet |\n",
    "|  2 | Forest     | Green |\n",
    "|  3 | HayField   | Red |\n",
    "|  4 | StrawField | Light Green |\n",
    "|  5 | DarkSoil   | Blue |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "\n",
    "### Read the data\n",
    "Open the training data, which contains the pixel values of the segmented regions shown in the image above. Each pixel contains 13 floating point values between 0 and 1, one for each channel.\n",
    "\n",
    "The training data is stored in a HDF5 file `Soderfjarden_training_data.h5`, it has following data sets:\n",
    "\n",
    " - `spectra`: a matrix, which dimensions are (23754, 15). Notice that two first values are the x and y coordinates of the pixel. Do not use them for classification, but use only following 13 channels. Construct a design matrix X (23754,13) by slicing this data.\n",
    " - `labels`: The expert segmented labels (23754, 1), the true values, the ground truth. This is the y-vector which you try to predict\n",
    " - `colnames`: The name of the columns (15,1), if you need them. Contains the band names, shown in he table above, plus the name of the x and y coordinate columns.\n",
    " - `wavelengths`: The wavelengts of each band (13,1), the same shown in the table above.\n",
    " - The HDF5 includes also a data attribute called `timestamp`, which contains the time when the image was acquired.\n",
    "\n",
    "If you can't remember how to read HDF5 file, refer to excercise 3.\n",
    "\n",
    "### Make training set and test set\n",
    "Separate your data X and y to training set (X_train, y_train) which contains 75% of the data and to the test set X_test, y_test which contains 25% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##>>> Some code for bootstrap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dae54fb501a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## >> Some tests, do not change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23754\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23754\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "## >> Some tests, do not change\n",
    "assert(X.shape==(23754,13))\n",
    "assert(y.shape==(23754,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "- Study the data, select a classifier, and train it using the training data.\n",
    "- Use cross validatin to test the performance of the classifier and tune it's parameters as good as you can\n",
    "- Finally test the classifier with the test set\n",
    "- Report the classification accuracy in the training set, cross validation score and accuracy in the test set\n",
    "- Plot also the confusion matrix when predicting the test set. \n",
    "- What is your opinion of the performance? Is there signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### >>>>> Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "### >>>> Some testing\n",
    "# Your classifier should reach above 90% of accuracy in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3, Select the best channels\n",
    "\n",
    "Like it often is, some features are more important for classification than the others. \n",
    "\n",
    "- Study which channels are the most important for the classification. You can use LASSO or Elastic Nets to select variables (SelectFromModel), as shown in the lecture notes of Module 6, or ir you use random forests (Extratrees, or boosted trees), they already keep account on most often used features, see also lecture notes of Module 6.\n",
    "- Plot of scatter plot of pixels in the training set, using two most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### >>> Some testing\n",
    "# It seems that there are only a few bands which are sufficient for classification. \n",
    "# Threfore you should get pretty nice separation of clusters in the scatter plot\n",
    "# You can also try to plot a scatter plot with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4, Extra task\n",
    "\n",
    "Now you have trained a classifier to perform well in the trainig data. It is time to put it in actual work.\n",
    " - Read the data of the whole image from another HDF5 file `Soderfjarden_image.h5`. It has the same structure as the training data file, except few differences\n",
    "   - There is no labels available, since the whole image is not labeled\n",
    "   - There is no pixel coordinates in the begining of spectra matrix, but only n samples and 13 channels\n",
    "   - There are also two additional attributes (width=924, height=630) which shows the image dimensions\n",
    " - Predict all values in the image, and store the results in vector y\n",
    " - Convert y to numerical, so that each class is described by a unique integer\n",
    " - Reshape y to image shape `image=y.reshape(630,924)`\n",
    " - Plot the image `plt.imshow(img)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###>>> Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##>>> SOme testing\n",
    "# You should see that all pixels are now assigned to some classes. You can compare with the original\n",
    "# image and try to find out if the classification looks good.\n",
    "# If it does not, how it could be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
